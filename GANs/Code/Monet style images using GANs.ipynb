{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9wwew3ipCSz",
        "outputId": "61cfea54-fa21-49c8-8f38-34c451d2676e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s9UWiW7kpFa_",
        "outputId": "4f68ad0b-3ee5-4cbb-f1fa-7a34ce6bbcea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: albumentations==1.3.0 in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (1.15.2)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (6.0.2)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.0) (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.13.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (10.4.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# 2. Install required libraries\n",
        "!pip install albumentations==1.3.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Be9UWiOpH8i"
      },
      "outputs": [],
      "source": [
        "# 3. Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-3iDQ8BYpK7l"
      },
      "outputs": [],
      "source": [
        "# 4. Define model architecture\n",
        "class DownConv(nn.Module):\n",
        "    def __init__(self, in_filters, out_filters):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=2, padding=1, padding_mode='reflect'),\n",
        "            nn.InstanceNorm2d(out_filters),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    def __init__(self, in_filters, out_filters):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_filters, out_filters, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(out_filters),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, channels, 3),\n",
        "            nn.InstanceNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, channels, 3),\n",
        "            nn.InstanceNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_channels, num_res=9):\n",
        "        super().__init__()\n",
        "        self.conv_1 = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, out_channels=64, kernel_size=7, padding=3, padding_mode='reflect'),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.down = nn.Sequential(\n",
        "            DownConv(64, 128),\n",
        "            DownConv(128, 256))\n",
        "\n",
        "        self.bottleneck = nn.Sequential(*[ResBlock(256) for _ in range(num_res)])\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            UpConv(256, 128),\n",
        "            UpConv(128, 64))\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(64, img_channels, kernel_size=7, stride=1, padding=3, padding_mode='reflect')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.down(x)\n",
        "        x = self.bottleneck(x)\n",
        "        x = self.up(x)\n",
        "        return torch.tanh(self.conv_2(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZVIjlxapN-Q"
      },
      "outputs": [],
      "source": [
        "# 5. Load model weights\n",
        "state_dict_path = '/content/drive/MyDrive/gen_monet_dict_1.pth'\n",
        "model = Generator(3)\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFd0sCN4pXJa"
      },
      "outputs": [],
      "source": [
        "# 6. Define transforms\n",
        "transform = A.Compose([\n",
        "    A.Resize(256, 256),  # resize for model compatibility\n",
        "    A.Normalize(mean=[0.5]*3, std=[0.5]*3, max_pixel_value=255.0),\n",
        "    ToTensorV2()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf4mnNVSqwQ0"
      },
      "outputs": [],
      "source": [
        "# 7. Define paths\n",
        "input_dir = '/content/drive/MyDrive/test_photos'\n",
        "output_dir = '/content/drive/MyDrive/monet_output'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 8. Convert and save all photos\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        path = os.path.join(input_dir, filename)\n",
        "        image = cv2.imread(path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        transformed = transform(image=image)['image'].unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(transformed)\n",
        "            output = output.squeeze().permute(1, 2, 0).numpy()\n",
        "            output = ((output + 1) * 127.5).astype(np.uint8)\n",
        "\n",
        "        out_path = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(out_path, cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-fidelity"
      ],
      "metadata": {
        "id": "ByGwUIyXgOxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_fidelity import calculate_metrics\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# Optional: Make sure both directories exist\n",
        "assert os.path.exists(input_dir), f\"{input_dir} does not exist!\"\n",
        "assert os.path.exists(output_dir), f\"{output_dir} does not exist!\"\n",
        "\n",
        "# Calculate FID\n",
        "metrics = calculate_metrics(\n",
        "    input1=input_dir,\n",
        "    input2=output_dir,\n",
        "    cuda=True,  # Set to False if you’re not using a GPU\n",
        "    isc=False,  # Inception Score not needed\n",
        "    fid=True    # Compute only FID\n",
        ")\n",
        "\n",
        "# Output the FID Score\n",
        "print( metrics['frechet_inception_distance'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHitAgQhfDzf",
        "outputId": "fc60def3-0ee2-4051-dd3d-c764632c8fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating feature extractor \"inception-v3-compat\" with features ['2048']\n",
            "Extracting statistics from input 1\n",
            "Looking for samples non-recursivelty in \"/content/drive/MyDrive/test_photos\" with extensions png,jpg,jpeg\n",
            "Found 400 samples, some are lossy-compressed - this may affect metrics\n",
            "Processing samples\n",
            "Extracting statistics from input 2\n",
            "Looking for samples non-recursivelty in \"/content/drive/MyDrive/monet_output\" with extensions png,jpg,jpeg\n",
            "Found 400 samples, some are lossy-compressed - this may affect metrics\n",
            "Processing samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "135.4182421594357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Frechet Inception Distance: 135.4182421594357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob_LdZW5qzdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc35277-cd43-4445-8809-78afac4beb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from typing import Tuple\n",
        "import random\n",
        "import os\n",
        "\n",
        "monet = \"/content/drive/MyDrive/monet_jpg\"\n",
        "photo =  \"/content/drive/MyDrive/photo_jpg\"\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 10  # Number of training epochs\n",
        "max_lr = 2e-4  # Maximum learning rate for scheduler\n",
        "initial_lr = max_lr / 10  # Initial learning rate for optimizers\n",
        "weight_decay = 0\n",
        "class ImageFolderCustom(Dataset):\n",
        "\n",
        "    def __init__(self, root: str, class_idx:int, max_size:int = -1, transform=None) -> None:\n",
        "        self.paths = [os.path.join(root, img) for img in os.listdir(root)]\n",
        "        random.shuffle(self.paths)\n",
        "        self.paths = self.paths[:max_size]\n",
        "        self.transform = transform\n",
        "        self.class_idx = class_idx\n",
        "\n",
        "    def load_image(self, index: int) -> Image.Image:\n",
        "        \"Opens an image via a path and returns it.\"\n",
        "        image_path = self.paths[index]\n",
        "        return Image.open(image_path)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"Returns the total number of samples.\"\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
        "        \"Returns one sample of data, data and label (X, y).\"\n",
        "        img = self.load_image(index)\n",
        "\n",
        "        if self.transform:\n",
        "            return self.transform(img), self.class_idx\n",
        "        else:\n",
        "            return img, self.class_idx\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "monet_dataset = ImageFolderCustom(root=monet,\n",
        "                                  class_idx=1,\n",
        "                                  max_size=32,\n",
        "                                  transform=data_transform)\n",
        "photo_dataset = ImageFolderCustom(root=photo,\n",
        "                                  class_idx=0,\n",
        "                                  transform=data_transform)\n",
        "monet_dl = DataLoader(dataset=monet_dataset,\n",
        "                      batch_size=1,\n",
        "                      num_workers=1,\n",
        "                      pin_memory=True,\n",
        "                      shuffle=True)\n",
        "photo_dl = DataLoader(dataset=photo_dataset,\n",
        "                      batch_size=1,\n",
        "                      num_workers=1,\n",
        "                      pin_memory=True,\n",
        "                      shuffle=True)\n",
        "\n",
        "def downsample(in_channels, out_channels, kernel_size, apply_instancenorm=True):\n",
        "    result = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size, stride=2, padding=1, bias=False))\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        result.append(nn.InstanceNorm2d(out_channels))\n",
        "\n",
        "    result.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    return result\n",
        "\n",
        "def upsample(in_channels, out_channels, kernel_size, apply_dropout=False):\n",
        "    result = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=2, padding=1),\n",
        "        nn.InstanceNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.append(nn.Dropout(0.5))\n",
        "\n",
        "    result.append(nn.ReLU())\n",
        "\n",
        "    return result\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.down_stack = nn.ModuleList([\n",
        "            downsample(3, 64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n",
        "            downsample(64, 128, 4), # (bs, 64, 64, 128)\n",
        "            downsample(128, 256, 4), # (bs, 32, 32, 256)\n",
        "            downsample(256,512, 4), # (bs, 16, 16, 512)\n",
        "            downsample(512,512, 4), # (bs, 8, 8, 512)\n",
        "            downsample(512,512, 4), # (bs, 4, 4, 512)\n",
        "            downsample(512,512, 4), # (bs, 2, 2, 512)\n",
        "            downsample(512,512, 4, apply_instancenorm=False)\n",
        "        ])\n",
        "\n",
        "        self.up_stack = nn.ModuleList([\n",
        "            upsample(512,512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "            upsample(1024,512, 4, apply_dropout=True), # (bs, 4, 4, 1024)1536\n",
        "            upsample(1024,512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "            upsample(1024,512, 4), # (bs, 16, 16, 1024)\n",
        "            upsample(1024,256, 4), # (bs, 32, 32, 512)\n",
        "            upsample(512,128, 4), # (bs, 64, 64, 256)\n",
        "            upsample(256,64, 4), # (bs, 128, 128, 128)\n",
        "        ])\n",
        "\n",
        "        self.last = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()) # (bs, 256, 256, 3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        skips = []\n",
        "        for down in self.down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips = reversed(skips[:-1])\n",
        "\n",
        "        for up, skip in zip(self.up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "\n",
        "        x = self.last(x)\n",
        "        return x\n",
        "monet_generator = Generator()\n",
        "photo_generator = Generator()\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequential = nn.Sequential(\n",
        "            downsample(3, 64, 4, apply_instancenorm=False),\n",
        "            downsample(64,128, 4),\n",
        "            downsample(128, 256, 4),\n",
        "            nn.ZeroPad2d(1),\n",
        "            nn.Conv2d(256, 512, 4, bias=False),\n",
        "            nn.InstanceNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ZeroPad2d(1),\n",
        "            nn.Conv2d(512, 1, 4),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sequential(x)\n",
        "        return x\n",
        "\n",
        "monet_discriminator = Discriminator()\n",
        "photo_discriminator = Discriminator()\n",
        "\n",
        "class CycleGAN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        monet_generator,\n",
        "        photo_generator,\n",
        "        monet_discriminator,\n",
        "        photo_discriminator,\n",
        "        generator_loss_fn,\n",
        "        discriminator_loss_fn,\n",
        "        calc_cycle_loss_fn,\n",
        "        identity_loss_fn,\n",
        "        lambda_cycle=10,\n",
        "    ):\n",
        "        super(CycleGAN, self).__init__()\n",
        "        self.monet_generator = monet_generator\n",
        "        self.photo_generator = photo_generator\n",
        "        self.monet_discriminator = monet_discriminator\n",
        "        self.photo_discriminator = photo_discriminator\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "\n",
        "        self.generator_loss_fn = generator_loss_fn\n",
        "        self.discriminator_loss_fn = discriminator_loss_fn\n",
        "        self.calc_cycle_loss_fn = calc_cycle_loss_fn\n",
        "        self.identity_loss_fn = identity_loss_fn\n",
        "\n",
        "    def train_step(self, X_monet, X_photo,\n",
        "                   monet_generator_optimizer,\n",
        "                   photo_generator_optimizer,\n",
        "                   monet_discriminator_optimizer,\n",
        "                   photo_discriminator_optimizer,\n",
        "                   device):\n",
        "\n",
        "        X_monet = X_monet.to(device)\n",
        "        X_photo = X_photo.to(device)\n",
        "\n",
        "        fake_monet = self.monet_generator(X_photo)\n",
        "        cycled_photo = self.photo_generator(fake_monet)\n",
        "\n",
        "        fake_photo = self.photo_generator(X_monet)\n",
        "        cycled_monet = self.monet_generator(fake_photo)\n",
        "\n",
        "        same_monet = self.monet_generator(X_monet)\n",
        "        same_photo = self.photo_generator(X_photo)\n",
        "\n",
        "        disc_X_monet = self.monet_discriminator(X_monet)\n",
        "        disc_fake_monet = self.monet_discriminator(fake_monet)\n",
        "        disc_cycled_monet = self.monet_discriminator(cycled_monet)\n",
        "\n",
        "        disc_X_photo = self.photo_discriminator(X_photo)\n",
        "        disc_fake_photo = self.photo_discriminator(fake_photo)\n",
        "        disc_cycled_photo = self.photo_discriminator(cycled_photo)\n",
        "\n",
        "        monet_gen_loss = self.generator_loss_fn(disc_fake_monet)\n",
        "        photo_gen_loss = self.generator_loss_fn(disc_fake_photo)\n",
        "\n",
        "        total_cycle_loss = self.calc_cycle_loss_fn(X_monet, cycled_monet, self.lambda_cycle) + self.calc_cycle_loss_fn(X_photo, cycled_photo, self.lambda_cycle)\n",
        "\n",
        "        total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(X_monet, same_monet, self.lambda_cycle)\n",
        "        total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(X_photo, same_photo, self.lambda_cycle)\n",
        "\n",
        "        monet_disc_loss = self.discriminator_loss_fn(disc_X_monet, disc_fake_monet)\n",
        "        photo_disc_loss = self.discriminator_loss_fn(disc_X_photo, disc_fake_photo)\n",
        "\n",
        "        monet_generator_optimizer.zero_grad()\n",
        "        photo_generator_optimizer.zero_grad()\n",
        "        monet_discriminator_optimizer.zero_grad()\n",
        "        photo_discriminator_optimizer.zero_grad()\n",
        "\n",
        "        total_monet_gen_loss.backward(retain_graph=True)\n",
        "        total_photo_gen_loss.backward(retain_graph=True)\n",
        "        monet_disc_loss.backward(retain_graph=True)\n",
        "        photo_disc_loss.backward(retain_graph=True)\n",
        "\n",
        "        grad_clip = 0.1\n",
        "        nn.utils.clip_grad_value_(monet_generator.parameters(), grad_clip)\n",
        "        nn.utils.clip_grad_value_(photo_generator.parameters(), grad_clip)\n",
        "        nn.utils.clip_grad_value_(monet_discriminator.parameters(), grad_clip)\n",
        "        nn.utils.clip_grad_value_(photo_discriminator.parameters(), grad_clip)\n",
        "\n",
        "        monet_generator_optimizer.step()\n",
        "        photo_generator_optimizer.step()\n",
        "        monet_discriminator_optimizer.step()\n",
        "        photo_discriminator_optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"monet_gen_loss\": total_monet_gen_loss,\n",
        "            \"photo_gen_loss\": total_photo_gen_loss,\n",
        "            \"monet_disc_loss\": monet_disc_loss,\n",
        "            \"photo_disc_loss\": photo_disc_loss\n",
        "        }\n",
        "\n",
        "BCEWLL = BCEWithLogitsLoss()\n",
        "\n",
        "def generator_loss(generated):\n",
        "    real_targets = torch.ones(generated.size(0), 1, 30, 30, device=device)\n",
        "    return BCEWLL(generated,real_targets)\n",
        "def discriminator_loss(real, generated):\n",
        "    real_targets = torch.ones(real.size(0), 1, 30, 30,device=device)\n",
        "    fake_targets = torch.zeros(generated.size(0), 1, 30, 30, device=device)\n",
        "\n",
        "    real_loss = BCEWLL(real,real_targets)\n",
        "    generated_loss = BCEWLL(generated,fake_targets)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss * 0.5\n",
        "def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n",
        "    loss1 = torch.mean(torch.abs(real_image - cycled_image))\n",
        "    return LAMBDA * loss1\n",
        "def identity_loss(real_image, same_image, LAMBDA):\n",
        "    loss = torch.mean(torch.abs(real_image - same_image))\n",
        "    return LAMBDA * 0.5 * loss\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, cycle_gan_model, monet_dl, photo_dl, device,\n",
        "                  m_gen_optimizer, p_gen_optimizer, m_disc_optimizer,p_disc_optimizer,\n",
        "                  weight_decay =0,start_idx = 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = {\"m_gen_lrs\" : [],\n",
        "               \"p_gen_lrs\" : [],\n",
        "               \"m_disc_lrs\" : [],\n",
        "               \"p_disc_lrs\" : []}\n",
        "\n",
        "    monet_gen_losses,photo_gen_losses,monet_disc_losses,photo_disc_losses = [],[],[],[]\n",
        "\n",
        "    m_gen_sched = torch.optim.lr_scheduler.OneCycleLR(m_gen_optimizer, max_lr, epochs=epochs, steps_per_epoch=len(monet_dl))\n",
        "    p_gen_sched = torch.optim.lr_scheduler.OneCycleLR(p_gen_optimizer, max_lr, epochs=epochs, steps_per_epoch=len(photo_dl))\n",
        "    m_disc_sched = torch.optim.lr_scheduler.OneCycleLR(m_disc_optimizer, max_lr, epochs=epochs, steps_per_epoch=len(monet_dl))\n",
        "    p_disc_sched = torch.optim.lr_scheduler.OneCycleLR(p_disc_optimizer, max_lr, epochs=epochs, steps_per_epoch=len(photo_dl))\n",
        "\n",
        "    cycle_gan_model.train()\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        torch.cuda.empty_cache()\n",
        "        combined_dl = zip(monet_dl,photo_dl)\n",
        "        total_monet_gen_loss = 0\n",
        "        total_photo_gen_loss = 0\n",
        "        monet_disc_loss = 0\n",
        "        photo_disc_loss = 0\n",
        "        for batch_idx, (real_images_monet, real_images_photo) in enumerate(combined_dl):\n",
        "            real_monets, _ = real_images_monet\n",
        "            real_photos, _ = real_images_photo\n",
        "\n",
        "            losses = cycle_gan_model.train_step(real_monets, real_photos, m_gen_optimizer, p_gen_optimizer,\n",
        "                                                m_disc_optimizer, p_disc_optimizer, device)\n",
        "\n",
        "            total_monet_gen_loss += losses[\"monet_gen_loss\"]\n",
        "            total_photo_gen_loss += losses[\"photo_gen_loss\"]\n",
        "            monet_disc_loss += losses[\"monet_disc_loss\"]\n",
        "            photo_disc_loss += losses[\"photo_disc_loss\"]\n",
        "\n",
        "            monet_gen_losses.append(losses[\"monet_gen_loss\"])\n",
        "            photo_gen_losses.append(losses[\"photo_gen_loss\"])\n",
        "            monet_disc_losses.append(losses[\"monet_disc_loss\"])\n",
        "            photo_disc_losses.append(losses[\"photo_disc_loss\"])\n",
        "\n",
        "            m_gen_sched.step()\n",
        "            p_gen_sched.step()\n",
        "            m_disc_sched.step()\n",
        "            p_disc_sched.step()\n",
        "\n",
        "            history[\"m_gen_lrs\"].append(get_lr(m_gen_optimizer))\n",
        "            history[\"p_gen_lrs\"].append(get_lr(p_gen_optimizer))\n",
        "            history[\"m_disc_lrs\"].append(get_lr(m_disc_optimizer))\n",
        "            history[\"p_disc_lrs\"].append(get_lr(p_disc_optimizer))\n",
        "\n",
        "\n",
        "        total_monet_gen_loss /= batch_idx\n",
        "        total_photo_gen_loss /= batch_idx\n",
        "        monet_disc_loss /= batch_idx\n",
        "        photo_disc_loss /= batch_idx\n",
        "\n",
        "        print(\"Epoch [{}/{}], total_monet_gen_loss: {:.4f}, total_photo_gen_loss: {:.4f}, monet_disc_loss: {:.4f}, photo_disc_loss: {:.4f}\".format(\n",
        "        epoch+1, epochs, total_monet_gen_loss, total_photo_gen_loss, monet_disc_loss, photo_disc_loss))\n",
        "    return history\n",
        "cycle_gan_model = CycleGAN(monet_generator, photo_generator,\n",
        "                 monet_discriminator, photo_discriminator,\n",
        "                 generator_loss,discriminator_loss,\n",
        "                 calc_cycle_loss,identity_loss).to(device)\n",
        "m_gen_optimizer = torch.optim.Adam(monet_generator.parameters(), max_lr, betas=(0.5, 0.999))\n",
        "p_gen_optimizer = torch.optim.Adam(photo_generator.parameters(), max_lr, betas=(0.5, 0.999))\n",
        "m_disc_optimizer = torch.optim.Adam(monet_discriminator.parameters(), max_lr, betas=(0.5, 0.999))\n",
        "p_disc_optimizer = torch.optim.Adam(photo_discriminator.parameters(), max_lr, betas=(0.5, 0.999))\n",
        "history = fit_one_cycle(epochs, max_lr, cycle_gan_model,\n",
        "                        monet_dl, photo_dl, device,\n",
        "                        m_gen_optimizer,\n",
        "                        p_gen_optimizer,\n",
        "                        m_disc_optimizer,\n",
        "                        p_disc_optimizer,\n",
        "                        )\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "5656a3f82f6649f282a7d43ebe376db1",
            "f70a232391b040f08f395825d7e88f53",
            "3fea0824c8ac43b7a0567a80927f49c3",
            "a1e93460a8574170b20d1b2025b8958e",
            "68ba365841454b8a98864204d8708a08",
            "bf7e806cff9641fe8c340c6cbcc86d9b",
            "033a1f2f66124e1e843e846f12aa02e2",
            "e169e8a6c41b4272bea31f0b02d0886c",
            "131ca9c36d0143fdb2295482c439886b",
            "7006d47b98894ffd948ee01571c3b18f",
            "a72ba00a68f748718e805e3bce39c459"
          ]
        },
        "id": "0KwKum_qO7rc",
        "outputId": "b4f3943b-b727-478c-db6b-0783a555ccc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5656a3f82f6649f282a7d43ebe376db1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], total_monet_gen_loss: 16.4679, total_photo_gen_loss: 16.3772, monet_disc_loss: 0.7688, photo_disc_loss: 0.7573\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 11207 has 14.73 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 11.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-46b70959a5ee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0mm_disc_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonet_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0mp_disc_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphoto_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m history = fit_one_cycle(epochs, max_lr, cycle_gan_model,\n\u001b[0m\u001b[1;32m    345\u001b[0m                         \u001b[0mmonet_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         \u001b[0mm_gen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-46b70959a5ee>\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(epochs, max_lr, cycle_gan_model, monet_dl, photo_dl, device, m_gen_optimizer, p_gen_optimizer, m_disc_optimizer, p_disc_optimizer, weight_decay, start_idx)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mreal_photos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images_photo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             losses = cycle_gan_model.train_step(real_monets, real_photos, m_gen_optimizer, p_gen_optimizer,\n\u001b[0m\u001b[1;32m    305\u001b[0m                                                 m_disc_optimizer, p_disc_optimizer, device)\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-46b70959a5ee>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, X_monet, X_photo, monet_generator_optimizer, photo_generator_optimizer, monet_discriminator_optimizer, photo_discriminator_optimizer, device)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mtotal_monet_gen_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mtotal_photo_gen_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mmonet_disc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mphoto_disc_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 11207 has 14.73 GiB memory in use. Of the allocated memory 14.60 GiB is allocated by PyTorch, and 11.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_photo = photo_dataset[random.randint(0,7083)][0]\n",
        "transforms.ToPILImage()(test_photo)\n",
        "output = cycle_gan_model.monet_generator(torch.reshape(test_photo, (1, 3, 256, 256)).to(device))\n",
        "transforms.ToPILImage()(torch.squeeze(output))\n"
      ],
      "metadata": {
        "id": "EXQatBd_PJG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "z1hdU4YRvMYb",
        "outputId": "8fd99799-3801-464b-fd3c-139cd707d87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-f7bad79af8c0>, line 322)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f7bad79af8c0>\"\u001b[0;36m, line \u001b[0;32m322\u001b[0m\n\u001b[0;31m    m_gen_optimizer = torch.optim.Adam(monet_generator.parameters(), lr=initial_lr, betas=(0.5, <true>.999), weight_decay=weight_decay)\u001b[0m\n\u001b[0m                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5656a3f82f6649f282a7d43ebe376db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f70a232391b040f08f395825d7e88f53",
              "IPY_MODEL_3fea0824c8ac43b7a0567a80927f49c3",
              "IPY_MODEL_a1e93460a8574170b20d1b2025b8958e"
            ],
            "layout": "IPY_MODEL_68ba365841454b8a98864204d8708a08"
          }
        },
        "f70a232391b040f08f395825d7e88f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7e806cff9641fe8c340c6cbcc86d9b",
            "placeholder": "​",
            "style": "IPY_MODEL_033a1f2f66124e1e843e846f12aa02e2",
            "value": " 10%"
          }
        },
        "3fea0824c8ac43b7a0567a80927f49c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e169e8a6c41b4272bea31f0b02d0886c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_131ca9c36d0143fdb2295482c439886b",
            "value": 1
          }
        },
        "a1e93460a8574170b20d1b2025b8958e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7006d47b98894ffd948ee01571c3b18f",
            "placeholder": "​",
            "style": "IPY_MODEL_a72ba00a68f748718e805e3bce39c459",
            "value": " 1/10 [00:16&lt;02:02, 13.63s/it]"
          }
        },
        "68ba365841454b8a98864204d8708a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7e806cff9641fe8c340c6cbcc86d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033a1f2f66124e1e843e846f12aa02e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e169e8a6c41b4272bea31f0b02d0886c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131ca9c36d0143fdb2295482c439886b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7006d47b98894ffd948ee01571c3b18f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a72ba00a68f748718e805e3bce39c459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}